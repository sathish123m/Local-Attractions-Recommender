{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import urllib\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip().strip('-')\n",
    "    text = text.lower()\n",
    "    text = '_'.join(text.split(' '))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(i,tree):\n",
    "    try:\n",
    "        container = tree.xpath(\"//div[@id='REVIEWS']//div[@class='listContainer']\")[0]\n",
    "        review_container = container.xpath(\".//div[@class='review-container']\")\n",
    "        for details in review_container:\n",
    "            user_container = details.xpath(\".//div[@class='ui_column is-2']\")[0]\n",
    "            user.append(clean_text(user_container.xpath(\".//div[@class='member_info']/div/div[2]/span/text()\")[0]))\n",
    "            rating_container = details.xpath(\".//div[@class='ui_column is-9']\")[0]\n",
    "            user_rating.append(float(clean_text(rating_container.xpath(\".//div[@class='rating reviewItemInline']/span/@class\")[0])[-2:])/10)    \n",
    "            review_date.append(rating_container.xpath(\".//div[@class='rating reviewItemInline']/span/@title\")[0])\n",
    "            review.append(rating_container.xpath(\".//div[contains(@class,'quote')]/span/text()\")[0]+'. '+rating_container.xpath(\".//p[@class='partial_entry']/text()\")[0])\n",
    "            attraction_id.append(i)\n",
    "\n",
    "        if container.xpath(\".//div[@class = 'unified ui_pagination ']/a[2]/@href\"):\n",
    "            sleep(5)\n",
    "            page_no = container.xpath(\".//div[@class = 'unified ui_pagination ']/a[2]/@data-page-number\")[0]\n",
    "            print(\"log: getting reviews for attraction \"+str(i)+\"- page \"+page_no)\n",
    "            link = \"https://tripadvisor.ca\"+container.xpath(\".//div[@class = 'unified ui_pagination ']/a[2]/@href\")[0]\n",
    "            page = requests.get(link)\n",
    "            html = page.content\n",
    "            tree = lh.fromstring(html)\n",
    "            get_reviews(i,tree)\n",
    "    \n",
    "    except:\n",
    "        error_file.write(str.encode(\"error: reviews for attraction \"+str(i)+\" could not be extracted\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(i,url, maps_key):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        html = page.content\n",
    "        tree = lh.fromstring(html)\n",
    "\n",
    "        geo = tree.xpath(\"//ul[@class = 'breadcrumbs']\")[0]\n",
    "        try:\n",
    "            country.append(clean_text(geo.xpath(\"./li[1]/a/span/text()\")[0]))\n",
    "            province.append(clean_text(geo.xpath(\"./li[2]/a/span/text()\")[0]))\n",
    "            city.append(clean_text(geo.xpath(\"./li[3]/a/span/text()\")[0]))\n",
    "        except:\n",
    "            country.append(\"canada\")\n",
    "            city.append(\"nil\")\n",
    "            province.append(\"nil\")   \n",
    "            \n",
    "        try:\n",
    "            address = tree.xpath(\"//div[@class='supplier']/a/text()\")[0]+\", \"+country[-1]\n",
    "            maps_api_url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "            request_url = maps_api_url + '?' + urllib.parse.urlencode({'address':address,'key':maps_key})\n",
    "            response = requests.get(request_url)\n",
    "            resp_json_payload = response.json()\n",
    "            location.append(resp_json_payload['results'][0]['geometry']['location'])\n",
    "        except:\n",
    "            location.append(\"nil\")\n",
    "        try:\n",
    "            details = tree.xpath(\"//div[@class='product_highlights_module']\")[0]\n",
    "            name.append(clean_text(details.xpath(\"./h1/text()\")[0]))\n",
    "        except:\n",
    "            name.append(\"nil\")\n",
    "        try:\n",
    "            rating.append(float(clean_text(details.xpath(\"./div[@class='rating_and_tag_wrapper']/div/span/@class\")[0])[-2:])/10)\n",
    "        except:\n",
    "            rating.append(float(-1))\n",
    "        try:\n",
    "            price.append(float(clean_text(details.xpath(\"./div[@class='product_cta_wrapper']//div[@class='price']/span/span/text()\")[0])[2:]))\n",
    "        except:\n",
    "            price.append(float(-1))\n",
    "        \n",
    "        att_id.append(i)\n",
    "        \n",
    "        print(\"log: getting reviews for attraction \"+str(i))\n",
    "        get_reviews(i,tree)\n",
    "    \n",
    "    except:\n",
    "        error_file.write(str.encode(\"error: Details of the attraction \"+str(i)+\" could not be extracted\\n\"))\n",
    "        error_file.write(str.encode(url+\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('outputs/attractions_cat.json',orient='records')\n",
    "df['attraction_id'] = df.index\n",
    "df = df.rename(index=str,columns={\"attraction\": \"url\"})\n",
    "\n",
    "att_id = list()\n",
    "country = list()\n",
    "province = list()\n",
    "city = list()\n",
    "location = list()\n",
    "name = list()\n",
    "rating = list()\n",
    "price = list()\n",
    "\n",
    "attraction_id = list()\n",
    "user = list()\n",
    "review = list()\n",
    "user_rating = list()\n",
    "review_date = list()\n",
    "\n",
    "error_file = open(\"outputs/error_log.txt\",\"wb\")\n",
    "for i in range(df.shape[0]):\n",
    "    print(\"log: collecting details for attraction \"+str(i))\n",
    "    extract_info(i,df['url'][i],'AIzaSyC2jxjbR_svb9EjCeMBivCNEcCaaxdEYIA')\n",
    "error_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Details dataframe verification:\")\n",
    "print(len(country))\n",
    "print(len(province))\n",
    "print(len(city))\n",
    "print(len(name))\n",
    "print(len(rating))\n",
    "print(len(price))\n",
    "print(len(location))\n",
    "att_df = pd.DataFrame({'attraction_id':att_id,\n",
    "                   'name':name,\n",
    "                   'country':country,\n",
    "                   'province':province,\n",
    "                   'city':city,\n",
    "                   'location':location,\n",
    "                   'price':price,\n",
    "                   'rating':rating})\n",
    "att_df.to_json('outputs/attractions_details_batch2.json',orient='records',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reviews dataframe verification\")\n",
    "print(len(attraction_id))\n",
    "print(len(user))\n",
    "print(len(review))\n",
    "print(len(user_rating))\n",
    "print(len(review_date))\n",
    "att_rev_df = pd.DataFrame({'attraction_id':attraction_id,\n",
    "                           'user':user,\n",
    "                           'rating':user_rating,\n",
    "                           'review':review,\n",
    "                           'review_date':review_date})\n",
    "\n",
    "att_rev_df.to_json('outputs/attractions_reviews_batch2.json',orient='records',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
